name: Weekly Scrape PR'HRAN

on:
  schedule:
    # 2x tedensko: Ponedeljek in ÄŒetrtek ob 00:20 CET
    - cron: '20 23 * * 1'  # Ponedeljek
    - cron: '20 23 * * 4'  # ÄŒetrtek

  # OmogoÄi roÄni zagon iz GitHub UI
  workflow_dispatch:

jobs:
  # Vzporedni scrape jobs za vsako trgovino
  scrape-spar:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 ure - brez limita
    defaults:
      run:
        working-directory: scraper
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scraper/requirements.txt'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps chromium

      - name: Setup credentials
        run: echo '${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}' > credentials.json

      - name: Scrape SPAR
        run: |
          mkdir -p output
          python -c "
          from stores.spar import SparScraper
          from playwright.sync_api import sync_playwright
          import json

          print('Starting SPAR scrape...')
          with sync_playwright() as p:
              browser = p.chromium.launch(headless=True)
              context = browser.new_context(viewport={'width': 1920, 'height': 1080})
              context.set_default_timeout(60000)
              page = context.new_page()

              scraper = SparScraper(page)
              products = scraper.scrape_all()

              with open('output/spar_products.json', 'w', encoding='utf-8') as f:
                  json.dump(products, f, ensure_ascii=False, indent=2)

              print(f'SPAR: {len(products)} products')
              browser.close()
          "
        env:
          PYTHONUNBUFFERED: "1"

      - uses: actions/upload-artifact@v4
        with:
          name: spar-products
          path: scraper/output/spar_products.json
          retention-days: 1

  scrape-mercator:
    runs-on: ubuntu-latest
    timeout-minutes: 240  # 4 ure - kategorije so hitrejÅ¡e
    defaults:
      run:
        working-directory: scraper
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scraper/requirements.txt'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps chromium

      - name: Scrape MERCATOR
        run: |
          mkdir -p output
          python -c "
          from stores.mercator import MercatorScraper
          from playwright.sync_api import sync_playwright
          import json

          print('Starting Mercator scrape...')
          with sync_playwright() as p:
              browser = p.chromium.launch(headless=True)
              context = browser.new_context(viewport={'width': 1920, 'height': 1080})
              context.set_default_timeout(60000)
              page = context.new_page()

              scraper = MercatorScraper(page)
              products = scraper.scrape_all_simple()

              with open('output/mercator_products.json', 'w', encoding='utf-8') as f:
                  json.dump(products, f, ensure_ascii=False, indent=2)

              print(f'Mercator: {len(products)} products')
              browser.close()
          "
        env:
          PYTHONUNBUFFERED: "1"

      - uses: actions/upload-artifact@v4
        with:
          name: mercator-products
          path: scraper/output/mercator_products.json
          retention-days: 1

  scrape-tus:
    runs-on: ubuntu-latest
    timeout-minutes: 180  # 3 ure - brez limita
    defaults:
      run:
        working-directory: scraper
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scraper/requirements.txt'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt
          playwright install chromium
          playwright install-deps chromium

      - name: Scrape TUS
        run: |
          mkdir -p output
          python -c "
          from stores.tus import TusScraper
          from playwright.sync_api import sync_playwright
          import json

          print('Starting Tus scrape...')
          with sync_playwright() as p:
              browser = p.chromium.launch(headless=True)
              context = browser.new_context(viewport={'width': 1920, 'height': 1080})
              context.set_default_timeout(60000)
              page = context.new_page()

              scraper = TusScraper(page)
              products = scraper.scrape_all()

              with open('output/tus_products.json', 'w', encoding='utf-8') as f:
                  json.dump(products, f, ensure_ascii=False, indent=2)

              print(f'Tus: {len(products)} products')
              browser.close()
          "
        env:
          PYTHONUNBUFFERED: "1"

      - uses: actions/upload-artifact@v4
        with:
          name: tus-products
          path: scraper/output/tus_products.json
          retention-days: 1

  # Kombinira rezultate in uploada v Convex
  combine-and-upload:
    needs: [scrape-spar, scrape-mercator, scrape-tus]
    if: always()  # ZaÅ¾eni tudi Äe en scraper odpove
    runs-on: ubuntu-latest
    timeout-minutes: 60  # VeÄ Äasa za upload

    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'
          cache-dependency-path: 'scraper/requirements.txt'

      - name: Install dependencies
        run: |
          pip install --upgrade pip
          pip install -r scraper/requirements.txt

      - name: Setup credentials
        run: echo '${{ secrets.GOOGLE_SHEETS_CREDENTIALS }}' > scraper/credentials.json

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: scraper/output
          merge-multiple: true

      - name: List downloaded files
        run: ls -la scraper/output/ || echo "No output directory"

      - name: Combine and upload to Convex
        run: |
          python -c "
          import json
          import os
          import requests
          from pathlib import Path
          from datetime import datetime

          print('='*60)
          print('COMBINING SCRAPED DATA')
          print('='*60)

          output_dir = Path('scraper/output')
          all_products = {}
          total = 0

          for store in ['spar', 'mercator', 'tus']:
              file_path = output_dir / f'{store}_products.json'
              if file_path.exists():
                  with open(file_path, 'r', encoding='utf-8') as f:
                      products = json.load(f)
                  all_products[store] = products
                  total += len(products)
                  print(f'{store.upper()}: {len(products)} products')
              else:
                  print(f'{store.upper()}: MISSING')
                  all_products[store] = []

          print(f'\\nTotal: {total} products')

          # Ustvari master_products.json (preprost format)
          # POMEMBNO: Convex ne sprejema null vrednosti, samo undefined
          # Zato filtriramo vse null vrednosti iz podatkov
          def clean_product(p, store):
              item = {'ime': p.get('ime', ''), 'trgovina': store.capitalize()}
              # Dodaj samo ne-null vrednosti
              if p.get('redna_cena') is not None:
                  item['redna_cena'] = p['redna_cena']
              if p.get('akcijska_cena') is not None:
                  item['akcijska_cena'] = p['akcijska_cena']
              if p.get('slika'):
                  item['slika'] = p['slika']
              if p.get('kategorija'):
                  item['kategorija'] = p['kategorija']
              if p.get('enota'):
                  item['enota'] = p['enota']
              return item

          master_products = []
          for store, products in all_products.items():
              for p in products:
                  master_products.append(clean_product(p, store))

          with open('scraper/output/master_products.json', 'w', encoding='utf-8') as f:
              json.dump(master_products, f, ensure_ascii=False, indent=2)
          print(f'Saved master_products.json ({len(master_products)} products)')

          # Upload to Convex
          convex_url = os.getenv('CONVEX_URL')
          token = os.getenv('PRHRAN_INGEST_TOKEN')

          if convex_url and token:
              print('\\nUploading to Convex...')
              batch_size = 200  # Manjsi batch za stabilnost
              uploaded = 0
              errors = 0

              for i in range(0, len(master_products), batch_size):
                  batch = master_products[i:i+batch_size]
                  # Retry logika
                  for attempt in range(3):
                      try:
                          resp = requests.post(
                              f'{convex_url}/api/ingest/grocery',
                              json={'items': batch},  # Brez clearFirst - samo posodobi
                              headers={'Authorization': f'Bearer {token}', 'Content-Type': 'application/json'},
                              timeout=180
                          )
                          if resp.status_code == 200:
                              uploaded += len(batch)
                              print(f'  Batch {i//batch_size + 1}: {len(batch)} OK')
                              break
                          else:
                              if attempt < 2:
                                  import time
                                  time.sleep(2)
                              else:
                                  print(f'  Batch {i//batch_size + 1}: ERROR {resp.status_code}')
                                  errors += 1
                      except Exception as e:
                          if attempt < 2:
                              import time
                              time.sleep(2)
                          else:
                              print(f'  Batch {i//batch_size + 1}: ERROR {e}')
                              errors += 1

              print(f'Uploaded: {uploaded}/{len(master_products)}')
          else:
              print('WARNING: CONVEX_URL or PRHRAN_INGEST_TOKEN not set')

          print('\\nDONE!')
          "
        env:
          PYTHONUNBUFFERED: "1"
          CONVEX_URL: ${{ secrets.PRHRAN_INGEST_URL }}
          PRHRAN_INGEST_TOKEN: ${{ secrets.PRHRAN_INGEST_TOKEN }}

      - name: Upload final results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: scrape-results-${{ github.run_number }}
          path: scraper/output/
          retention-days: 14

      - name: Generate Summary
        if: always()
        run: |
          echo "## ðŸ›’ PR'HRAN Daily Scrape Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "**Run Date:** $(date '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ -f scraper/output/master_products.json ]; then
            TOTAL=$(python3 -c "import json; data=json.load(open('scraper/output/master_products.json')); print(len(data))")
            echo "### âœ… Success" >> $GITHUB_STEP_SUMMARY
            echo "- **Total products:** $TOTAL" >> $GITHUB_STEP_SUMMARY
          else
            echo "### âŒ Error" >> $GITHUB_STEP_SUMMARY
            echo "No master_products.json found" >> $GITHUB_STEP_SUMMARY
          fi

          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Per Store" >> $GITHUB_STEP_SUMMARY
          for store in spar mercator tus; do
            FILE="scraper/output/${store}_products.json"
            if [ -f "$FILE" ]; then
              COUNT=$(python3 -c "import json; print(len(json.load(open('$FILE'))))")
              echo "- âœ… **${store^^}:** $COUNT products" >> $GITHUB_STEP_SUMMARY
            else
              echo "- âŒ **${store^^}:** FAILED" >> $GITHUB_STEP_SUMMARY
            fi
          done
