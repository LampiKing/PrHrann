name: Daily Product Scraper - Fully Automated

on:
  schedule:
    # Run every day at 6 AM CET
    - cron: '0 5 * * *'
  workflow_dispatch: {}

jobs:
  scrape-and-upload:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          pip install playwright beautifulsoup4 requests undetected-chromedriver selenium scrapy
          playwright install chromium

      - name: Try Method 1 - Playwright Scraper
        id: playwright
        continue-on-error: true
        run: |
          python grocery_scanner.py
          if [ -f "cene_data/trenutne_cene.json" ]; then
            PRODUCTS=$(jq '.stevilo_izdelkov' cene_data/trenutne_cene.json)
            if [ "$PRODUCTS" -gt 50 ]; then
              echo "success=true" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          echo "success=false" >> $GITHUB_OUTPUT

      - name: Try Method 2 - Undetected ChromeDriver
        if: steps.playwright.outputs.success != 'true'
        id: undetected
        continue-on-error: true
        run: |
          python undetected_scraper.py
          if [ -f "cene_data/trenutne_cene.json" ]; then
            PRODUCTS=$(jq '.stevilo_izdelkov' cene_data/trenutne_cene.json)
            if [ "$PRODUCTS" -gt 50 ]; then
              echo "success=true" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          echo "success=false" >> $GITHUB_OUTPUT

      - name: Try Method 3 - Scrapy Framework
        if: steps.undetected.outputs.success != 'true'
        id: scrapy
        continue-on-error: true
        run: |
          python scrapy_scraper.py
          # Merge scrapy outputs
          python -c "
import json, os
products = []
for f in ['cene_data/spar_products.json', 'cene_data/mercator_products.json']:
    if os.path.exists(f):
        with open(f) as file:
            data = json.load(file)
            products.extend(data.get('izdelki', []))
if products:
    with open('cene_data/trenutne_cene.json', 'w') as out:
        json.dump({'stevilo_izdelkov': len(products), 'izdelki': products}, out)
"
          if [ -f "cene_data/trenutne_cene.json" ]; then
            PRODUCTS=$(jq '.stevilo_izdelkov' cene_data/trenutne_cene.json)
            if [ "$PRODUCTS" -gt 50 ]; then
              echo "success=true" >> $GITHUB_OUTPUT
              exit 0
            fi
          fi
          echo "success=false" >> $GITHUB_OUTPUT

      - name: Fallback - Generate Sample Database
        if: steps.scrapy.outputs.success != 'true'
        run: |
          python api_mass_import.py

      - name: Upload to Convex
        env:
          CONVEX_URL: ${{ secrets.CONVEX_URL }}
        run: |
          # Install Node dependencies
          npm install
          
          # Transform and upload
          python -c "
import json, subprocess, sys
with open('cene_data/trenutne_cene.json') as f:
    data = json.load(f)
    
products = []
for item in data['izdelki']:
    products.append({
        'productName': item['ime'],
        'storeName': item['trgovina'],
        'price': item['redna_cena'],
        'salePrice': item.get('akcijska_cena') or item['redna_cena'],
        'lastUpdated': '$(date -I)'
    })

# Upload in small batches to avoid command line length issues
BATCH_SIZE = 10
for i in range(0, len(products), BATCH_SIZE):
    batch = products[i:i+BATCH_SIZE]
    args = json.dumps({'products': batch})
    try:
        subprocess.run(['npx', 'convex', 'run', 'products:bulkUpsert', '--prod', args], check=True)
        print(f'Uploaded batch {i//BATCH_SIZE + 1}')
    except:
        print(f'Failed batch {i//BATCH_SIZE + 1}')
"

      - name: Notify on failure
        if: failure()
        run: |
          echo "::warning::Daily scraping failed - check logs"
